# ğŸ¥ Web Tutorial Script: Scrapy Crash Course with VS Code
## ğŸ”´ INTRO
---
**Narration:**

> ***"Hey everyone! In this tutorial, Iâ€™ll show you how to go from zero to scraping with **Scrapy** using **VS Code**. Weâ€™ll install Scrapy, set up a project, write a few spiders, and export the data in different formats."***

---
<br><br><br>
<br><br><br>


## **What Is Scrapy**

> ***So what is Scrapy ?***  
>***Scrapy** is an **open-source web crawling and web scraping framework** written in **Python**. It's widely used for extracting structured data from websites, which makes it useful for things like:*
>
>* **Data mining**
>* **Price monitoring**
>* **Lead generation**
>* **Market research**
>* **News aggregation**
____
<br><br><br>
<br><br><br>

## ğŸ”§ **Key Features of Scrapy**
<br><br><br>

____
> **So what are the key features of Scrapy ?**  
> Scrapy has many features, these include.
>* **Fast and asynchronous**: *(Asynchronous means not happening at the same time,)* Built on top of **Twisted**, an asynchronous networking framework, which allows high-performance scraping.  
>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;âœ… **"Built on top of Twisted"**  
>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*     *Twisted is a Python framework specifically designed for handling network communication (like HTTP requests).*    
>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*     *Saying Scrapy is "built on top of Twisted" means Scrapy uses Twisted as its core engine for handling web requests and responses.*
> 
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;âœ… "An asynchronous networking framework"      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Asynchronous means Scrapy can send many requests at once without waiting for each one to finish before starting the next.    
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Instead of saying â€œsend request â†’ wait â†’ process â†’ repeat,â€ it says:    
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    * ***â€œsend request â†’ send another â†’ send another â†’ handle them all as they finish.â€***    
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This makes Scrapy very fast and efficient, especially when scraping hundreds or thousands of pages.
>   
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;âœ… "Which allows high-performance scraping"  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Because of Twistedâ€™s asynchronous nature, Scrapy can scrape websites much faster than traditional scripts that wait for each response one at a time.  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is why Scrapy is used in large-scale scraping jobs â€” it's designed for speed and performance.  
>
>* **Selectors based on XPath or CSS**: Helps extract data from HTML or XML documents.
>* **Built-in support for following links**: Useful for crawling through pages automatically.
>* **Item pipelines**: Clean, validate, and store scraped data (e.g., to databases or files).  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* âœ… **What are Pipelines,Explanation:**  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*ğŸ§¼ Clean    
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Once Scrapy gathers data, it may need some touch-up â€” removing whitespace, fixing encoding issues, or formatting dates. This is the "cleaning" stage.  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* âœ… Validate  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*This step checks if the data is complete and correct. For example, if a field like price should always be a number, this step makes sure thatâ€™s true â€” and can discard or fix bad data.  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*ğŸ’¾ Store  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    After cleaning and validating, the data is stored somewhere â€” like:  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    A CSV file  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    A JSON file  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    A SQL or NoSQL database  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    Or even sent to an API  
>  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*ğŸ§  In Context of Scrapy:  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    Scrapy uses Item Pipelines to run all these steps after the spider extracts the data.  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    You define a pipeline in pipelines.py, and Scrapy runs each item through it in order.  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*ğŸ’¡ Analogy:  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Think of your scraped data like raw vegetables from a garden.  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Pipelines are your kitchen:  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    ğŸ§¼ You wash (clean) them  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    âœ… You inspect (validate) them  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    ğŸ± You store (save) them in containers  
>
> 
>* **Middleware system**: Allows custom behavior for request/response handling.
>* **Shell and logging**: Useful for testing and debugging.


____
<br><br><br>
<br><br><br>

## ğŸ— Basic Scrapy Workflow

1. **Create a Scrapy project**
2. **Define an item** (structure of the data you want)
3. **Write a spider** (crawler that sends requests and parses responses)
4. **Process items via pipelines**
5. **Store/export the data** (CSV, JSON, database, etc.)
<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

---
<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

# 1ï¸âƒ£ Creating a Scrapy Project

Narration:

***"Now letâ€™s create our first Scrapy project."***

**Terminal Commands:**  
ğŸ“„ Script:
```bash
# Create a new Scrapy project
scrapy startproject quotes_scraper

# Move into the project directory
cd quotes_scraper

```

### ğŸ“**Project Structure:**

>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;quotes_scraper/  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ”€â”€ scrapy.cfg  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â””â”€â”€ quotes_scraper/  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ”€â”€ __init__.py  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ”€â”€ items.py      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ”€â”€ middlewares.py      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ”€â”€ pipelines.py      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ”€â”€ settings.py      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â””â”€â”€ spiders/      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â””â”€â”€ __init__.py  

## Installing Scrapy in a Virtual Environment

Narration:
> ***"Letâ€™s start by setting up a virtual environment and installing Scrapy."***

**Terminal Commands:**

```python
# Create and activate a virtual environment
python -m venv scrapy_env

# Windows\scrapy_env\Scripts\activate
# macOS/Linux
source scrapy_env/bin/activate

# Install Scrapy
pip install scrapy
```

ğŸ“Œ Show VS Code Terminal and demonstrate each command. Open VS Code inside the folder:

```
code .
```


---

## ğŸ“ What Is an Environment in Python,and Why It Matters?

**Narration:**

> "Before we dive into coding, letâ€™s talk about something important â€” environments."

### âœ… What Is a Python Environment?

> "A Python environment is like a *workspace* for your code. It contains everything your project needs to run â€” including Python itself, and any libraries or tools your code depends on."

### ğŸ§ª Think of it like:

> "Imagine you're baking. Each recipe might need different ingredients â€” some recipes need chocolate chips, some need cinnamon, and some need none at all. A **Python environment** is like having a separate pantry for each recipe so that ingredients donâ€™t get mixed up or go bad."

---

## â“ Why Do We Need Virtual Environments?

### 1. **Avoid Conflicts**

> "Different projects often need different versions of libraries. For example, one project might use Scrapy version 2.5, while another needs version 2.10. A virtual environment keeps these separate, so one project doesnâ€™t break the other."

### 2. **Keep Things Organized**

> "All dependencies for a project are stored in one folder. That makes it easy to manage, share, and reproduce the setup â€” especially when you share your code with someone else."

### 3. **Safe Testing**

> "You can test new packages or code safely without affecting your main Python installation."

---

## ğŸ›  What Happens When You Create an Environment?

> "When you run this command:"

```bash
python -m venv myenv
```

> "Python creates a folder called `myenv` that contains a fresh copy of Python and a blank slate for installing packages."

---

## ğŸ Final Summary

> "So, a Python environment helps keep your projects clean, organized, and conflict-free. Itâ€™s a best practice to create a new environment for every project â€” especially when working with tools like Scrapy."

____

### **Narration:**

>"Letâ€™s start by setting up a virtual environment and installing Scrapy."

## **Terminal Commands:**


```bash
# Create and activate a virtual environment
python -m venv scrapy_env

# Windows\scrapy_env\Scripts\activate
# macOS/Linux
source scrapy_env/bin/activate

# Install Scrapy
pip install scrapy
```

**VS Code Tip:**
Open VS Code inside the project folder:

```bash
code .
```
____


<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

### 2ï¸âƒ£**Define an item**  


## ğŸ“¦ What Is an Item in Scrapy?

**Narration:**

> *"As your spider crawls the web and collects data, it needs a way to structure that data consistently. Thatâ€™s where **Items** come in."*

### âœï¸ What Is an Item?

> *"An **Item** in Scrapy is like a container or a template for the data you scrape. It defines the fields you expect to collect, such as title, price, author, or URL."*

### ğŸ›  How Do You Define One?

> *"You define an Item by creating a Python class that inherits from `scrapy.Item`, and then you define each field using `scrapy.Field()`."*

### ğŸ¤– Analogy

> *"Think of an item like a form or a spreadsheet column. It says: 'â€˜Hereâ€™s the kind of data I expect,â€™ and Scrapy fills it in as the spider runs."*

### ğŸ“ƒ Example: Defining an Item

In `items.py`:

```python
import scrapy

class QuoteItem(scrapy.Item):
    text = scrapy.Field()
    author = scrapy.Field()
    tags = scrapy.Field()
```

### âœ… Why Use Items?

> "Using Items helps you keep your data organized, enforce structure, and make your code cleaner. Itâ€™s especially useful when exporting data or sending it through pipelines."

---
<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

## 3ï¸âƒ£ **Write a spider**

### ğŸ” What Is a Spider in Scrapy?

Narration:
> ***"Next up, letâ€™s talk about one of the most important parts of Scrapy: the spider."***

## ğŸ•·ï¸ What Is a Spider?

 **"A spider in Scrapy is a Python class that defines how to crawl a website and what data to extract. Itâ€™s like a blueprint for where to go and what to grab."**

Visual Aid: (Display a flow diagram of spider -> website -> extracted data)

"Each spider needs at least:

>- **A name**
>
>- **A list of URLs to start from**
>
>- **A method to parse the downloaded content"**

## ğŸ¤– A Spider Is Like...

### *"Think of it like a robot you program. You tell it:*

- **Where to begin**

- **How to find the data**

- **What to do with it"**

## ğŸ“– Example: Basic Spider

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = ['http://quotes.toscrape.com']

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                'text': quote.css("span.text::text").get(),
                'author': quote.css("small.author::text").get(),
                'tags': quote.css("div.tags a.tag::text").getall(),
            }
```

## 4ï¸âƒ£ Running the Spider  


**Narration:**

> "Now, letâ€™s run the spider and export the data to a JSON file."

**Terminal Command:**

```bash
scrapy crawl quotes -o quotes.json
```

**Result:** Show `quotes.json` in VS Code.

---

## âœ… What Happens When It Runs?

"When you run this spider with the `scrapy crawl` command, Scrapy will:

- **Starts at the URL you give it**

- **Downloads the HTML**

- **Calls the parse function with the response**

- **Collects and outputs whatever data your spider yields"**


____

<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

    
## **Process items via pipelines**






## 5ï¸âƒ£ Following Links to Scrape Author Details

**Narration:**

> "Letâ€™s enhance our spider to follow links and grab author bios."

**Update Spider:**

```python
def parse(self, response):
    for quote in response.css("div.quote"):
        author_url = quote.css("a::attr(href)").get()
        yield response.follow(author_url, self.parse_author)

def parse_author(self, response):
    yield {
        'name': response.css("h3.author-title::text").get().strip(),
        'birthdate': response.css("span.author-born-date::text").get(),
        'bio': response.css("div.author-description::text").get().strip()
    }
```

---

## 6ï¸âƒ£ Using XPath Selectors (Alternate Example)

**Narration:**

> â€œYou can also use XPath instead of CSS for scraping.â€

**Example:**

```python
def parse(self, response):
    for quote in response.xpath("//div[@class='quote']"):
        yield {
            'text': quote.xpath("span[@class='text']/text()").get(),
            'author': quote.xpath("span/small[@class='author']/text()").get(),
            'tags': quote.xpath("div[@class='tags']/a[@class='tag']/text()").getall()
        }
```

---

## 7ï¸âƒ£ Exporting Data to CSV

**Narration:**

> "You can also export to CSV or XML with one command."

**Command:**

```bash
scrapy crawl quotes -o quotes.csv
```

---

## 8ï¸âƒ£ Using Scrapy Shell for Testing Selectors

**Narration:**

> "Use the Scrapy shell to test selectors interactively."

**Commands:**

```bash
scrapy shell http://quotes.toscrape.com
```

Then try:

```python
response.css("span.text::text").getall()
response.xpath("//span[@class='text']/text()").getall()
```

---

## 9ï¸âƒ£ Bonus: Pagination

**Narration:**

> â€œLetâ€™s scrape all pages by following the â€˜Nextâ€™ button.â€

**Update Spider:**

```python
def parse(self, response):
    for quote in response.css("div.quote"):
        yield {
            'text': quote.css("span.text::text").get(),
            'author': quote.css("small.author::text").get(),
            'tags': quote.css("div.tags a.tag::text").getall(),
        }

    next_page = response.css("li.next a::attr(href)").get()
    if next_page:
        yield response.follow(next_page, self.parse)
```

---

## ğŸ”š Wrap Up

**Narration:**

> â€œAnd thatâ€™s a quick crash course on Scrapy with VS Code! You learned how to install Scrapy, set up a project, write spiders, follow links, and export data in multiple formats.â€

> â€œIf this helped you, give it a like and subscribe. Check the description for the full code and extra resources.â€
