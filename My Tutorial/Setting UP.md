# üé• Web Tutorial Script: Scrapy Crash Course with VS Code

<br><br><br>
## üî¥ INTRO

**Narration:**
> ***"Hey everyone! In this tutorial, I‚Äôll show you how to go from zero to scraping with **Scrapy** using **VS Code**. We‚Äôll install Scrapy, set up a project, write a few spiders, and export the data in different formats."***
---
<br><br>
## **What Is Scrapy**<br>
**Narration:**<br>
> ***Scrapy is an open-source web crawling and web scraping framework written in Python. It's widely used for extracting structured data from websites, which makes it useful for things like:***<br>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* **Data mining**<br> 
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* **Price monitoring**<br>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* **Lead generation**<br>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* **Market research**<br>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* **News aggregation**<br>
---
<br>

## üîß **Key Features of Scrapy**

**Narration:**<br>
> **So what are the key features of Scrapy ?** <br> 
> Scrapy has many features, these include.<br>
> * **Fast and asynchronous**: Built on top of **Twisted**, an asynchronous networking framework, which allows high-performance scraping.<br>
>* **Selectors based on XPath or CSS**: Helps extract data from HTML or XML documents.<br> 
>* **Built-in support for following links** Useful for crawling through pages automatically.<br> 
>* **Item pipelines**: Clean, validate, and store scraped data (e.g., to databases or files).<br> 
>* **Middleware system**: Allows custom behavior for request/response handling.<br> 
>* **Shell and logging**:  Useful for testing and debugging.<br> 
---
>***Lets break that down a bit.***  
>* **Fast and asynchronous**: *(Asynchronous means not happening at the same time,)* Built on top of **Twisted**, an asynchronous networking framework, which allows high-performance scraping.  
>    >  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚úÖ **"Built on top of Twisted"**  
>    >  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*     *Twisted is a Python framework specifically designed for handling network communication (like HTTP requests).*    
>    >  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*     *Saying Scrapy is "built on top of Twisted" means Scrapy uses Twisted as its core engine for handling web requests and responses.*
>    >
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚úÖ **"An asynchronous networking framework"**      
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Asynchronous means Scrapy can send many requests at once without waiting for each one to finish before starting the next.    
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Instead of saying ‚Äúsend request ‚Üí wait ‚Üí process ‚Üí repeat,‚Äù it says:    
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* ***‚Äúsend request ‚Üí send another ‚Üí send another ‚Üí handle them all as they finish.‚Äù***    
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This makes Scrapy very fast and efficient, especially when scraping hundreds or thousands of pages.
>    >   
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚úÖ "Which allows high-performance scraping"  
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Because of Twisted‚Äôs asynchronous nature, Scrapy can scrape websites much faster than traditional scripts that wait for each response one at a time.  
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is why Scrapy is used in large-scale scraping jobs ‚Äî it's designed for speed and performance.  
>
>* **Selectors based on XPath or CSS**: Helps extract data from HTML or XML documents.<br>
> ‚ú® **What Does this Sentence Mean?**<br>
>    >###    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; üîé What Are Selectors in Scrapy?<br>
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Narration:** <br>
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Scrapy needs a way to find and extract specific data from a web page ‚Äî that‚Äôs where **selectors** come in."<br><br><br>
>    >    Let‚Äôs break that down:<br><br>
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **‚ÄúSelectors based on XPath or CSS: Helps extract data from HTML or XML documents.‚Äù**
>    >###    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚úÖ What Is a Selector?
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "A **selector** is like a smart address system. It tells Scrapy where to look in a web page‚Äôs HTML or XML."
>    >### üß≠ XPath and CSS ‚Äî Two Ways to Navigate a Page
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. **CSS Selectors**
>    >    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "These use the same rules you‚Äôd write in CSS to style a webpage ‚Äî like `div.quote > span.text`."
>   >
>    >2. **XPath Expressions**

   > "XPath is a language for navigating XML and HTML trees. It‚Äôs more powerful and precise, especially when things get complex ‚Äî like `//div[@class='quote']/span[@class='text']`."

---

### üìÉ Example in Code

```python
# Using CSS
quote.css("span.text::text").get()

# Using XPath
quote.xpath("span[@class='text']/text()").get()
```

‚û°Ô∏è Both of these extract the quote text from inside a `<span>` tag.

---

### ü§ñ Analogy

> "Imagine an HTML document is a huge family tree.
>
> * **CSS selectors** are like saying, 'give me all the cousins named Emily.'
> * **XPath** is like saying, 'go to the second uncle‚Äôs second daughter if her name is Emily.'"

---

### üèÅ Final Summary

> "Selectors ‚Äî whether CSS or XPath ‚Äî are how Scrapy finds and pulls out the exact pieces of data you care about, like titles, prices, authors, or links."

---

Let me know if you'd like a deeper comparison of CSS vs XPath or want this added to the full script.

>* **Built-in support for following links**: Useful for crawling through pages automatically.
>* **Item pipelines**: Clean, validate, and store scraped data (e.g., to databases or files).  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* ‚úÖ **What are Pipelines,Explanation:**  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*üßº Clean    
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Once Scrapy gathers data, it may need some touch-up ‚Äî removing whitespace, fixing encoding issues, or formatting dates. This is the "cleaning" stage.  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* ‚úÖ Validate  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*This step checks if the data is complete and correct. For example, if a field like price should always be a number, this step makes sure that‚Äôs true ‚Äî and can discard or fix bad data.  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*üíæ Store  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    After cleaning and validating, the data is stored somewhere ‚Äî like:  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    A CSV file  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    A JSON file  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    A SQL or NoSQL database  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    Or even sent to an API  
>  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*üß† In Context of Scrapy:  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    Scrapy uses Item Pipelines to run all these steps after the spider extracts the data.  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    You define a pipeline in pipelines.py, and Scrapy runs each item through it in order.  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*üí° Analogy:  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Think of your scraped data like raw vegetables from a garden.  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Pipelines are your kitchen:  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    üßº You wash (clean) them  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    ‚úÖ You inspect (validate) them  
>
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*    üç± You store (save) them in containers  
>
> 
>* **Middleware system**: Allows custom behavior for request/response handling.
>* **Shell and logging**: Useful for testing and debugging.


____
<br><br><br>
<br><br><br>

## üèó Basic Scrapy Workflow

1. **Create a Scrapy project**
2. **Define an item** (structure of the data you want)
3. **Write a spider** (crawler that sends requests and parses responses)
4. **Process items via pipelines**
5. **Store/export the data** (CSV, JSON, database, etc.)
<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

---
<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

# 1Ô∏è‚É£ Creating a Scrapy Project

Narration:

***"Now let‚Äôs create our first Scrapy project."***

**Terminal Commands:**  
üìÑ Script:
```bash
# Create a new Scrapy project
scrapy startproject quotes_scraper

# Move into the project directory
cd quotes_scraper

```

### üìù**Project Structure:**

>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;quotes_scraper/  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îú‚îÄ‚îÄ scrapy.cfg  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îî‚îÄ‚îÄ quotes_scraper/  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îú‚îÄ‚îÄ __init__.py  
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îú‚îÄ‚îÄ items.py      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îú‚îÄ‚îÄ middlewares.py      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îú‚îÄ‚îÄ pipelines.py      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îú‚îÄ‚îÄ settings.py      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îî‚îÄ‚îÄ spiders/      
>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îî‚îÄ‚îÄ __init__.py  

## Installing Scrapy in a Virtual Environment

Narration:
> ***"Let‚Äôs start by setting up a virtual environment and installing Scrapy."***

**Terminal Commands:**

```python
# Create and activate a virtual environment
python -m venv scrapy_env

# Windows\scrapy_env\Scripts\activate
# macOS/Linux
source scrapy_env/bin/activate

# Install Scrapy
pip install scrapy
```

üìå Show VS Code Terminal and demonstrate each command. Open VS Code inside the folder:

```
code .
```


---

## üéì What Is an Environment in Python,and Why It Matters?

**Narration:**

> "Before we dive into coding, let‚Äôs talk about something important ‚Äî environments."

### ‚úÖ What Is a Python Environment?

> "A Python environment is like a *workspace* for your code. It contains everything your project needs to run ‚Äî including Python itself, and any libraries or tools your code depends on."

### üß™ Think of it like:

> "Imagine you're baking. Each recipe might need different ingredients ‚Äî some recipes need chocolate chips, some need cinnamon, and some need none at all. A **Python environment** is like having a separate pantry for each recipe so that ingredients don‚Äôt get mixed up or go bad."

---

## ‚ùì Why Do We Need Virtual Environments?

### 1. **Avoid Conflicts**

> "Different projects often need different versions of libraries. For example, one project might use Scrapy version 2.5, while another needs version 2.10. A virtual environment keeps these separate, so one project doesn‚Äôt break the other."

### 2. **Keep Things Organized**

> "All dependencies for a project are stored in one folder. That makes it easy to manage, share, and reproduce the setup ‚Äî especially when you share your code with someone else."

### 3. **Safe Testing**

> "You can test new packages or code safely without affecting your main Python installation."

---

## üõ† What Happens When You Create an Environment?

> "When you run this command:"

```bash
python -m venv myenv
```

> "Python creates a folder called `myenv` that contains a fresh copy of Python and a blank slate for installing packages."

---

## üèÅ Final Summary

> "So, a Python environment helps keep your projects clean, organized, and conflict-free. It‚Äôs a best practice to create a new environment for every project ‚Äî especially when working with tools like Scrapy."

____

### **Narration:**

>"Let‚Äôs start by setting up a virtual environment and installing Scrapy."

## **Terminal Commands:**


```bash
# Create and activate a virtual environment
python -m venv scrapy_env

# Windows\scrapy_env\Scripts\activate
# macOS/Linux
source scrapy_env/bin/activate

# Install Scrapy
pip install scrapy
```

**VS Code Tip:**
Open VS Code inside the project folder:

```bash
code .
```
____


<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

### 2Ô∏è‚É£**Define an item**  


## üì¶ What Is an Item in Scrapy?

**Narration:**

> *"As your spider crawls the web and collects data, it needs a way to structure that data consistently. That‚Äôs where **Items** come in."*

### ‚úèÔ∏è What Is an Item?

> *"An **Item** in Scrapy is like a container or a template for the data you scrape. It defines the fields you expect to collect, such as title, price, author, or URL."*

### üõ† How Do You Define One?

> *"You define an Item by creating a Python class that inherits from `scrapy.Item`, and then you define each field using `scrapy.Field()`."*

### ü§ñ Analogy

> *"Think of an item like a form or a spreadsheet column. It says: '‚ÄòHere‚Äôs the kind of data I expect,‚Äô and Scrapy fills it in as the spider runs."*

### üìÉ Example: Defining an Item

In `items.py`:

```python
import scrapy

class QuoteItem(scrapy.Item):
    text = scrapy.Field()
    author = scrapy.Field()
    tags = scrapy.Field()
```

### ‚úÖ Why Use Items?

> "Using Items helps you keep your data organized, enforce structure, and make your code cleaner. It‚Äôs especially useful when exporting data or sending it through pipelines."

---
<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

## 3Ô∏è‚É£ **Write a spider**

### üîç What Is a Spider in Scrapy?

Narration:
> ***"Next up, let‚Äôs talk about one of the most important parts of Scrapy: the spider."***

## üï∑Ô∏è What Is a Spider?

 **"A spider in Scrapy is a Python class that defines how to crawl a website and what data to extract. It‚Äôs like a blueprint for where to go and what to grab."**

Visual Aid: (Display a flow diagram of spider -> website -> extracted data)

"Each spider needs at least:

>- **A name**
>
>- **A list of URLs to start from**
>
>- **A method to parse the downloaded content"**

## ü§ñ A Spider Is Like...

### *"Think of it like a robot you program. You tell it:*

- **Where to begin**

- **How to find the data**

- **What to do with it"**

## üìñ Example: Basic Spider

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = ['http://quotes.toscrape.com']

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                'text': quote.css("span.text::text").get(),
                'author': quote.css("small.author::text").get(),
                'tags': quote.css("div.tags a.tag::text").getall(),
            }
```

## 4Ô∏è‚É£ Running the Spider  


**Narration:**

> "Now, let‚Äôs run the spider and export the data to a JSON file."

**Terminal Command:**

```bash
scrapy crawl quotes -o quotes.json
```

**Result:** Show `quotes.json` in VS Code.

---

## ‚úÖ What Happens When It Runs?

"When you run this spider with the `scrapy crawl` command, Scrapy will:

- **Starts at the URL you give it**

- **Downloads the HTML**

- **Calls the parse function with the response**

- **Collects and outputs whatever data your spider yields"**


____

<br><br><br>
<br><br><br>
<br><br><br>
<br><br><br>

    
## **Process items via pipelines**






## 5Ô∏è‚É£ Following Links to Scrape Author Details

**Narration:**

> "Let‚Äôs enhance our spider to follow links and grab author bios."

**Update Spider:**

```python
def parse(self, response):
    for quote in response.css("div.quote"):
        author_url = quote.css("a::attr(href)").get()
        yield response.follow(author_url, self.parse_author)

def parse_author(self, response):
    yield {
        'name': response.css("h3.author-title::text").get().strip(),
        'birthdate': response.css("span.author-born-date::text").get(),
        'bio': response.css("div.author-description::text").get().strip()
    }
```

---

## 6Ô∏è‚É£ Using XPath Selectors (Alternate Example)

**Narration:**

> ‚ÄúYou can also use XPath instead of CSS for scraping.‚Äù

**Example:**

```python
def parse(self, response):
    for quote in response.xpath("//div[@class='quote']"):
        yield {
            'text': quote.xpath("span[@class='text']/text()").get(),
            'author': quote.xpath("span/small[@class='author']/text()").get(),
            'tags': quote.xpath("div[@class='tags']/a[@class='tag']/text()").getall()
        }
```

---

## 7Ô∏è‚É£ Exporting Data to CSV

**Narration:**

> "You can also export to CSV or XML with one command."

**Command:**

```bash
scrapy crawl quotes -o quotes.csv
```

---

## 8Ô∏è‚É£ Using Scrapy Shell for Testing Selectors

**Narration:**

> "Use the Scrapy shell to test selectors interactively."

**Commands:**

```bash
scrapy shell http://quotes.toscrape.com
```

Then try:

```python
response.css("span.text::text").getall()
response.xpath("//span[@class='text']/text()").getall()
```

---

## 9Ô∏è‚É£ Bonus: Pagination

**Narration:**

> ‚ÄúLet‚Äôs scrape all pages by following the ‚ÄòNext‚Äô button.‚Äù

**Update Spider:**

```python
def parse(self, response):
    for quote in response.css("div.quote"):
        yield {
            'text': quote.css("span.text::text").get(),
            'author': quote.css("small.author::text").get(),
            'tags': quote.css("div.tags a.tag::text").getall(),
        }

    next_page = response.css("li.next a::attr(href)").get()
    if next_page:
        yield response.follow(next_page, self.parse)
```

---

## üîö Wrap Up

**Narration:**

> ‚ÄúAnd that‚Äôs a quick crash course on Scrapy with VS Code! You learned how to install Scrapy, set up a project, write spiders, follow links, and export data in multiple formats.‚Äù

> ‚ÄúIf this helped you, give it a like and subscribe. Check the description for the full code and extra resources.‚Äù
